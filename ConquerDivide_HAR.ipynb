{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "stf5T1ID9HMP",
    "outputId": "7383140a-1aa1-449b-f454-22324efbb7ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,CuDNNLSTM,BatchNormalization\n",
    "from keras.layers import Conv1D,MaxPooling1D,Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def dump_file(filename, mode, data):\n",
    "    '''\n",
    "    Save model on the disk\n",
    "    '''\n",
    "    pickle.dump(data, open(filename, mode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ldi8ORcpP5NH"
   },
   "source": [
    "## Load the Signals (Input Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Features: 561\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# get the features from the file features.txt\n",
    "features = list()\n",
    "with open('UCI_HAR_Dataset/features.txt') as f:\n",
    "    features = [line.split()[1] for line in f.readlines()]\n",
    "print('No of Features: {}'.format(len(features)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data signals\n",
    "# Signals are from Accelerometer and Gyroscope\n",
    "# The signals are in x,y,z directions\n",
    "# Sensor signals are filtered to have only body acceleration\n",
    "# excluding the acceleration due to gravity\n",
    "# Triaxial acceleration from the accelerometer is total acceleration\n",
    "SIGNALS = [\n",
    "    \"body_acc_x\",\n",
    "    \"body_acc_y\",\n",
    "    \"body_acc_z\",\n",
    "    \"body_gyro_x\",\n",
    "    \"body_gyro_y\",\n",
    "    \"body_gyro_z\",\n",
    "    \"total_acc_x\",\n",
    "    \"total_acc_y\",\n",
    "    \"total_acc_z\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activities are the class labels\n",
    "# It is a 6 class classification\n",
    "ACTIVITIES = {\n",
    "    0: 'WALKING',\n",
    "    1: 'WALKING_UPSTAIRS',\n",
    "    2: 'WALKING_DOWNSTAIRS',\n",
    "    3: 'SITTING',\n",
    "    4: 'STANDING',\n",
    "    5: 'LAYING',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to read the data from csv file\n",
    "def _read_csv(filename):\n",
    "    return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
    "\n",
    "# Utility function to load the load\n",
    "def load_signals(subset):\n",
    "    signals_data = []\n",
    "\n",
    "    for signal in SIGNALS:\n",
    "        filename = f'UCI_HAR_Dataset/{subset}/Inertial Signals/{signal}_{subset}.txt'\n",
    "        signals_data.append(\n",
    "            _read_csv(filename).to_numpy()\n",
    "        ) \n",
    "\n",
    "    # Transpose is used to change the dimensionality of the output,\n",
    "    # aggregating the signals by combination of sample/timestep.\n",
    "    # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
    "    return np.transpose(signals_data, (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def load_y_static_dynamic(subset):\n",
    "        \"\"\"\n",
    "        The objective that we are trying to predict is a integer, from 1 to 6,\n",
    "        that represents a human activity. We return a binary representation of \n",
    "        every sample objective as a 6 bits vector using One Hot Encoding\n",
    "        (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "        \"\"\"\n",
    "        filename = f'UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n",
    "        y = _read_csv(filename)[0]\n",
    "        y[y<=3] = 0\n",
    "        y[y>3] = 1\n",
    "        return pd.get_dummies(y).to_numpy()\n",
    "    \n",
    "def load_data_static_dynamic():\n",
    "    '''\n",
    "    Load train, test data and scale the data as well\n",
    "    '''\n",
    "    X_train_2c, X_val_2c = load_signals('train'), load_signals('test')\n",
    "    Y_train_2c, Y_val_2c = load_y_static_dynamic('train'), load_y_static_dynamic('test')\n",
    "    \n",
    "    # fit and transform data\n",
    "    Scale = fit(X_train_2c)\n",
    "    dump_file('Scale_2class.p','wb', Scale)\n",
    "    X_train_2c = transform(X_train_2c, Scale)\n",
    "    X_val_2c = transform(X_val_2c, Scale)\n",
    "    \n",
    "    return X_train_2c, Y_train_2c, X_val_2c, Y_val_2c \n",
    "\n",
    "def transform(X, scale):\n",
    "    '''\n",
    "    Transform the data\n",
    "    '''\n",
    "    temp_X1 = X.reshape((X.shape[0] * X.shape[1], X.shape[2]))\n",
    "    temp_X1 = scale.transform(temp_X1)\n",
    "    return temp_X1.reshape(X.shape)\n",
    "\n",
    "def fit(X):\n",
    "    '''\n",
    "    Fit data for scaling\n",
    "    '''\n",
    "    # remove overlaping\n",
    "    remove = int(X.shape[1] / 2)\n",
    "    temp_X = X[:, -remove:, :]\n",
    "    # flatten data\n",
    "    temp_X = temp_X.reshape((temp_X.shape[0] * temp_X.shape[1], temp_X.shape[2]))\n",
    "    scale = StandardScaler()\n",
    "    scale.fit(temp_X)\n",
    "    return scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2c, Y_train_2c, X_val_2c,  Y_val_2c = load_data_static_dynamic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 128, 9)\n",
      "(2947, 128, 9)\n",
      "(7352, 2)\n",
      "(2947, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_2c.shape)\n",
    "print(X_val_2c.shape)\n",
    "print(Y_train_2c.shape)\n",
    "print(Y_val_2c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QIH4YF43GI3F"
   },
   "source": [
    "### Fucntion for Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQv6ulQ-GQdM"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "\n",
    "# Utility function to print the confusion matrix\n",
    "def confusion_matrix_cnn(Y_true, Y_pred,activities):\n",
    "    Y_true = pd.Series([activities[y] for y in np.argmax(Y_true, axis=1)])\n",
    "    Y_pred = pd.Series([activities[y] for y in np.argmax(Y_pred, axis=1)])\n",
    "\n",
    "    #return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])\n",
    "    return confusion_matrix(Y_true, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RokpWzLcyag2"
   },
   "source": [
    "## Loading the Output labels by spliting into Static and Dynamic \n",
    "\n",
    "\n",
    "1.  walking, up, down -- dynamic\n",
    "2.   sitting standing lying -- static \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing tensorflow\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring a session\n",
    "session_conf = tf.ConfigProto(\n",
    "    intra_op_parallelism_threads=1,\n",
    "    inter_op_parallelism_threads=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Keras\n",
    "from keras import backend as K\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to count the number of classes\n",
    "def _count_classes(y):\n",
    "    return len(set([tuple(category) for category in y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update LSTM layers\n",
    "n_hidden_1 = 32\n",
    "n_hidden_2 = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FqYLCV38zrVv"
   },
   "source": [
    "## Model for classifying data into Static and Dynamic activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/mayank171986/Human-Activity-Detection/blob/master/human-activity-detection.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 126, 32)           896       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 124, 32)           3104      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 124, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 62, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1984)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                99250     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 103,352\n",
      "Trainable params: 103,352\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)\n",
    "# Start Session\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "K.set_session(sess)\n",
    "# Create model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu',kernel_initializer='he_uniform',input_shape=(128,9)))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu',kernel_initializer='he_uniform'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 0.0360 - acc: 0.9869 - val_loss: 0.0746 - val_acc: 0.9783\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 63s 9ms/step - loss: 0.0042 - acc: 0.9989 - val_loss: 0.0294 - val_acc: 0.9936\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 6.4344e-04 - acc: 0.9999 - val_loss: 0.0125 - val_acc: 0.9973\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 1.2485e-04 - acc: 1.0000 - val_loss: 0.0175 - val_acc: 0.9966\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 67s 9ms/step - loss: 1.4892e-04 - acc: 1.0000 - val_loss: 0.0207 - val_acc: 0.9963\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.0072 - acc: 0.9984 - val_loss: 0.0100 - val_acc: 0.9986\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 2.0327e-05 - acc: 1.0000 - val_loss: 0.0091 - val_acc: 0.9990\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 9s 1ms/step - loss: 1.3286e-06 - acc: 1.0000 - val_loss: 0.0091 - val_acc: 0.9990\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 1.9687e-06 - acc: 1.0000 - val_loss: 0.0089 - val_acc: 0.9990\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 1.9564e-06 - acc: 1.0000 - val_loss: 0.0090 - val_acc: 0.9990\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 3.8553e-06 - acc: 1.0000 - val_loss: 0.0082 - val_acc: 0.9990\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 9.0197e-07 - acc: 1.0000 - val_loss: 0.0086 - val_acc: 0.9990\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.0075 - acc: 0.9989 - val_loss: 0.0026 - val_acc: 0.9986\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 3.0977e-04 - acc: 0.9997 - val_loss: 0.0033 - val_acc: 0.9990\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 2.0082e-05 - acc: 1.0000 - val_loss: 0.0032 - val_acc: 0.9990\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.0011 - acc: 0.9997 - val_loss: 0.0060 - val_acc: 0.9986\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 1.3465e-07 - acc: 1.0000 - val_loss: 0.0060 - val_acc: 0.9986\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 9s 1ms/step - loss: 8.3655e-07 - acc: 1.0000 - val_loss: 0.0057 - val_acc: 0.9986\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 9s 1ms/step - loss: 1.3062e-07 - acc: 1.0000 - val_loss: 0.0057 - val_acc: 0.9986\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 1.3181e-07 - acc: 1.0000 - val_loss: 0.0057 - val_acc: 0.9986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d3a1e890f0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train_2c,Y_train_2c, epochs=20, batch_size=16,validation_data=(X_val_2c, Y_val_2c), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_accuracy 1.0 test_accuracy 0.998642687478792\n"
     ]
    }
   ],
   "source": [
    "_,acc_val = model.evaluate(X_val_2c,Y_val_2c,verbose=0)\n",
    "_,acc_train = model.evaluate(X_train_2c,Y_train_2c,verbose=0)\n",
    "print('Train_accuracy',acc_train,'test_accuracy',acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k23RYA0l0QEW"
   },
   "source": [
    "## Save the 2 class classification model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "##saving model\n",
    "model.save('final_model_2class.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nP87LG0u0j3j"
   },
   "source": [
    "## Classificaton of Static activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_y_static(subset):\n",
    "        \"\"\"\n",
    "        The objective that we are trying to predict is a integer, from 1 to 6,\n",
    "        that represents a human activity. We return a binary representation of \n",
    "        every sample objective as a 6 bits vector using One Hot Encoding\n",
    "        (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "        \"\"\"\n",
    "        filename = f'UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n",
    "        y = _read_csv(filename)[0]\n",
    "        y_subset = y>3\n",
    "        y = y[y_subset]\n",
    "        return pd.get_dummies(y).to_numpy(),y_subset\n",
    "    \n",
    "def load_data_static():\n",
    "    '''\n",
    "    Load train, test data and scale the data as well\n",
    "    '''\n",
    "    Y_train_s, y_train_sub = load_y_static('train')\n",
    "    Y_val_s, y_test_sub = load_y_static('test')\n",
    "    \n",
    "    X_train_s, X_val_s = load_signals('train'), load_signals('test')\n",
    "    X_train_s = X_train_s[y_train_sub]\n",
    "    X_val_s = X_val_s[y_test_sub]\n",
    "    \n",
    "    # fit and transform data\n",
    "    Scale = None\n",
    "    Scale = fit(X_train_s)\n",
    "    dump_file('Scale_static.p','wb', Scale)\n",
    "    X_train_s = transform(X_train_s, Scale)\n",
    "    X_val_s = transform(X_val_s, Scale)\n",
    "    \n",
    "    return X_train_s, Y_train_s, X_val_s, Y_val_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_s, Y_train_s, X_val_s,  Y_val_s = load_data_static()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape of train data (4067, 128, 9) Y shape (4067, 3)\n",
      "X Shape of val data (1560, 128, 9) Y shape (1560, 3)\n"
     ]
    }
   ],
   "source": [
    "print('X Shape of train data',X_train_s.shape, 'Y shape', Y_train_s.shape)\n",
    "print('X Shape of val data',X_val_s.shape,'Y shape',Y_val_s.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D_0o7yTB_X4m"
   },
   "source": [
    "## Model for Static Activities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 122, 64)           4096      \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 120, 32)           6176      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 120, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 40, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                38430     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 93        \n",
      "=================================================================\n",
      "Total params: 48,795\n",
      "Trainable params: 48,795\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Clear session\n",
    "K.clear_session()\n",
    "# Random seed\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)\n",
    "# Start session\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "K.set_session(sess)\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=7, activation='relu',kernel_initializer='he_uniform',input_shape=(128,9)))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu',kernel_initializer='he_uniform'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples\n",
      "Epoch 1/20\n",
      "4067/4067 [==============================] - 8s 2ms/step - loss: 0.4232 - acc: 0.8480 - val_loss: 0.3000 - val_acc: 0.8731\n",
      "Epoch 2/20\n",
      "4067/4067 [==============================] - 6s 2ms/step - loss: 0.1956 - acc: 0.9184 - val_loss: 0.2661 - val_acc: 0.8936\n",
      "Epoch 3/20\n",
      "4067/4067 [==============================] - 12s 3ms/step - loss: 0.1658 - acc: 0.9312 - val_loss: 0.2919 - val_acc: 0.8705\n",
      "Epoch 4/20\n",
      "4067/4067 [==============================] - 7s 2ms/step - loss: 0.1749 - acc: 0.9289 - val_loss: 0.3007 - val_acc: 0.8962\n",
      "Epoch 5/20\n",
      "4067/4067 [==============================] - 14s 3ms/step - loss: 0.1388 - acc: 0.9454 - val_loss: 0.2798 - val_acc: 0.8878\n",
      "Epoch 6/20\n",
      "4067/4067 [==============================] - 12s 3ms/step - loss: 0.1358 - acc: 0.9452 - val_loss: 0.2305 - val_acc: 0.9276\n",
      "Epoch 7/20\n",
      "4067/4067 [==============================] - 24s 6ms/step - loss: 0.1374 - acc: 0.9471 - val_loss: 0.2337 - val_acc: 0.9141\n",
      "Epoch 8/20\n",
      "4067/4067 [==============================] - 22s 6ms/step - loss: 0.1198 - acc: 0.9503 - val_loss: 0.2678 - val_acc: 0.8994\n",
      "Epoch 9/20\n",
      "4067/4067 [==============================] - 22s 5ms/step - loss: 0.1001 - acc: 0.9582 - val_loss: 0.2387 - val_acc: 0.9321\n",
      "Epoch 10/20\n",
      "4067/4067 [==============================] - 23s 6ms/step - loss: 0.1193 - acc: 0.9555 - val_loss: 0.2570 - val_acc: 0.9231\n",
      "Epoch 11/20\n",
      "4067/4067 [==============================] - 23s 6ms/step - loss: 0.1138 - acc: 0.9582 - val_loss: 0.2125 - val_acc: 0.9333\n",
      "Epoch 12/20\n",
      "4067/4067 [==============================] - 18s 4ms/step - loss: 0.0933 - acc: 0.9656 - val_loss: 0.2326 - val_acc: 0.9263\n",
      "Epoch 13/20\n",
      "4067/4067 [==============================] - 6s 1ms/step - loss: 0.0910 - acc: 0.9671 - val_loss: 0.1895 - val_acc: 0.9429\n",
      "Epoch 14/20\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.0890 - acc: 0.9666 - val_loss: 0.2075 - val_acc: 0.9359\n",
      "Epoch 15/20\n",
      "4067/4067 [==============================] - 6s 1ms/step - loss: 0.0680 - acc: 0.9749 - val_loss: 0.1895 - val_acc: 0.9391\n",
      "Epoch 16/20\n",
      "4067/4067 [==============================] - 10s 2ms/step - loss: 0.0830 - acc: 0.9685 - val_loss: 0.1985 - val_acc: 0.9365\n",
      "Epoch 17/20\n",
      "4067/4067 [==============================] - 28s 7ms/step - loss: 0.0578 - acc: 0.9752 - val_loss: 0.1765 - val_acc: 0.9423\n",
      "Epoch 18/20\n",
      "4067/4067 [==============================] - 21s 5ms/step - loss: 0.0552 - acc: 0.9806 - val_loss: 0.2349 - val_acc: 0.9147\n",
      "Epoch 19/20\n",
      "4067/4067 [==============================] - 6s 1ms/step - loss: 0.0877 - acc: 0.9698 - val_loss: 0.2149 - val_acc: 0.9340\n",
      "Epoch 20/20\n",
      "4067/4067 [==============================] - 8s 2ms/step - loss: 0.0459 - acc: 0.9808 - val_loss: 0.1936 - val_acc: 0.9346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d3a3588c18>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train_s,Y_train_s, epochs=20, batch_size=32,validation_data=(X_val_s, Y_val_s), verbose=1)\n",
    "# K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_accuracy 0.9857388738627981 test_accuracy 0.9346153846153846\n"
     ]
    }
   ],
   "source": [
    "_,acc_val = model.evaluate(X_val_s, Y_val_s,verbose=0)\n",
    "_,acc_train = model.evaluate(X_train_s,Y_train_s,verbose=0)\n",
    "print('Train_accuracy',acc_train,'test_accuracy',acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "##saving model\n",
    "model.save('final_model_static.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_y_dynamic(subset):\n",
    "    \"\"\"\n",
    "    The objective that we are trying to predict is a integer, from 1 to 6,\n",
    "    that represents a human activity. We return a binary representation of \n",
    "    every sample objective as a 6 bits vector using One Hot Encoding\n",
    "    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "    \"\"\"\n",
    "    filename = f'UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n",
    "    y = _read_csv(filename)[0]\n",
    "    y_subset = y<=3\n",
    "    y = y[y_subset]\n",
    "    return pd.get_dummies(y).to_numpy(),y_subset\n",
    "    \n",
    "def load_data_dynamic():\n",
    "    '''\n",
    "    Load train, test data and scale the data as well\n",
    "    '''\n",
    "    Y_train_d, y_train_sub = load_y_dynamic('train')\n",
    "    Y_val_d, y_test_sub = load_y_dynamic('test')\n",
    "    \n",
    "    X_train_d, X_val_d = load_signals('train'), load_signals('test')\n",
    "    X_train_d = X_train_d[y_train_sub]\n",
    "    X_val_d = X_val_d[y_test_sub]\n",
    "    \n",
    "    # fit and transform data\n",
    "    Scale = None\n",
    "    Scale = fit(X_train_d)\n",
    "    dump_file('Scale_dynamic.p','wb', Scale)\n",
    "    X_train_d = transform(X_train_d, Scale)\n",
    "    X_val_d = transform(X_val_d, Scale)\n",
    "    \n",
    "    return X_train_d, Y_train_d, X_val_d, Y_val_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_d, Y_train_d, X_val_d,  Y_val_d = load_data_dynamic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X shape (3285, 128, 9) Test X shape (1387, 128, 9)\n",
      "Train Y shape (3285, 3) Test Y shape (1387, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Train X shape',X_train_d.shape,'Test X shape',X_val_d.shape)\n",
    "print('Train Y shape',Y_train_d.shape,'Test Y shape',Y_val_d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X shape (3285, 128, 9) Test X shape (1387, 128, 9)\n",
      "Train Y shape (3285, 3) Test Y shape (1387, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Train X shape',X_train_d.shape,'Test X shape',X_val_d.shape)\n",
    "print('Train Y shape',Y_train_d.shape,'Test Y shape',Y_val_d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U7IS3w_2-QS5"
   },
   "source": [
    "## Model for Dynamic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3285 samples, validate on 1387 samples\n",
      "Epoch 1/20\n",
      "3285/3285 [==============================] - 7s 2ms/step - loss: 0.8125 - acc: 0.7260 - val_loss: 0.4112 - val_acc: 0.8659\n",
      "Epoch 2/20\n",
      "3285/3285 [==============================] - 4s 1ms/step - loss: 0.1114 - acc: 0.9623 - val_loss: 0.2984 - val_acc: 0.8904\n",
      "Epoch 3/20\n",
      "3285/3285 [==============================] - 6s 2ms/step - loss: 0.0435 - acc: 0.9857 - val_loss: 0.1599 - val_acc: 0.9430\n",
      "Epoch 4/20\n",
      "3285/3285 [==============================] - 13s 4ms/step - loss: 0.0211 - acc: 0.9936 - val_loss: 0.1888 - val_acc: 0.9221\n",
      "Epoch 5/20\n",
      "3285/3285 [==============================] - 6s 2ms/step - loss: 0.0131 - acc: 0.9951 - val_loss: 0.1602 - val_acc: 0.9402\n",
      "Epoch 6/20\n",
      "3285/3285 [==============================] - 13s 4ms/step - loss: 0.0122 - acc: 0.9951 - val_loss: 0.0874 - val_acc: 0.9683\n",
      "Epoch 7/20\n",
      "3285/3285 [==============================] - 5s 1ms/step - loss: 0.0033 - acc: 0.9991 - val_loss: 0.0939 - val_acc: 0.9661\n",
      "Epoch 8/20\n",
      "3285/3285 [==============================] - 5s 1ms/step - loss: 0.0034 - acc: 0.9985 - val_loss: 0.1045 - val_acc: 0.9704\n",
      "Epoch 9/20\n",
      "3285/3285 [==============================] - 4s 1ms/step - loss: 0.0019 - acc: 0.9994 - val_loss: 0.1290 - val_acc: 0.9596\n",
      "Epoch 10/20\n",
      "3285/3285 [==============================] - 4s 1ms/step - loss: 0.0032 - acc: 0.9988 - val_loss: 0.1004 - val_acc: 0.9654\n",
      "Epoch 11/20\n",
      "3285/3285 [==============================] - 5s 1ms/step - loss: 0.0013 - acc: 0.9997 - val_loss: 0.0822 - val_acc: 0.9726\n",
      "Epoch 12/20\n",
      "3285/3285 [==============================] - 5s 1ms/step - loss: 5.3048e-04 - acc: 1.0000 - val_loss: 0.1107 - val_acc: 0.9690\n",
      "Epoch 13/20\n",
      "3285/3285 [==============================] - 4s 1ms/step - loss: 0.0069 - acc: 0.9973 - val_loss: 0.0966 - val_acc: 0.9704\n",
      "Epoch 14/20\n",
      "3285/3285 [==============================] - 4s 1ms/step - loss: 0.0024 - acc: 0.9994 - val_loss: 0.0753 - val_acc: 0.9798\n",
      "Epoch 15/20\n",
      "3285/3285 [==============================] - 6s 2ms/step - loss: 0.0172 - acc: 0.9942 - val_loss: 0.1140 - val_acc: 0.9596\n",
      "Epoch 16/20\n",
      "3285/3285 [==============================] - 10s 3ms/step - loss: 0.0081 - acc: 0.9973 - val_loss: 0.1101 - val_acc: 0.9668\n",
      "Epoch 17/20\n",
      "3285/3285 [==============================] - 12s 4ms/step - loss: 0.0017 - acc: 0.9997 - val_loss: 0.1460 - val_acc: 0.9697\n",
      "Epoch 18/20\n",
      "3285/3285 [==============================] - 6s 2ms/step - loss: 0.0032 - acc: 0.9985 - val_loss: 0.0696 - val_acc: 0.9712\n",
      "Epoch 19/20\n",
      "3285/3285 [==============================] - 11s 3ms/step - loss: 4.2772e-04 - acc: 1.0000 - val_loss: 0.0678 - val_acc: 0.9769\n",
      "Epoch 20/20\n",
      "3285/3285 [==============================] - 7s 2ms/step - loss: 0.0034 - acc: 0.9994 - val_loss: 0.1442 - val_acc: 0.9676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d3a3a42eb8>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train_d,Y_train_d, epochs=20, batch_size=32,validation_data=(X_val_d, Y_val_d), verbose=1)\n",
    "# K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_BQgnibMFG3y"
   },
   "source": [
    "## Output For Dynamic Activities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_accuracy 1.0 test_accuracy 0.9675558759913482\n"
     ]
    }
   ],
   "source": [
    "_,acc_val = model.evaluate(X_val_d, Y_val_d,verbose=0)\n",
    "_,acc_train = model.evaluate(X_train_d,Y_train_d,verbose=0)\n",
    "print('Train_accuracy',acc_train,'test_accuracy',acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "##saving model\n",
    "model.save('final_model_dynamic.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_y_whole_data(subset):\n",
    "    \"\"\"\n",
    "    The objective that we are trying to predict is a integer, from 1 to 6,\n",
    "    that represents a human activity. We return a binary representation of \n",
    "    every sample objective as a 6 bits vector using One Hot Encoding\n",
    "    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "    \"\"\"\n",
    "    filename = f'UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n",
    "    y = _read_csv(filename)[0]\n",
    "    return y\n",
    "    \n",
    "def load_whole_data():    \n",
    "    '''\n",
    "    Load and split whole data\n",
    "    '''\n",
    "    X_train, X_val = load_signals('train'), load_signals('test')\n",
    "    Y_train, Y_val = load_y_whole_data('train'), load_y_whole_data('test')\n",
    "    \n",
    "    return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_val, Y_val = load_whole_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train X (7352, 128, 9) shape of train Y (7352,)\n",
      "shape of test X (2947, 128, 9) shape of test Y (2947,)\n"
     ]
    }
   ],
   "source": [
    "print('shape of train X',X_train.shape, 'shape of train Y',Y_train.shape)\n",
    "print('shape of test X', X_val.shape, 'shape of test Y', Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yHXrq_J2uDy6"
   },
   "source": [
    "## Final Prediction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading keras models and picle files for scaling data \n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "model_2class = load_model('final_model_2class.h5')\n",
    "model_dynamic = load_model('final_model_dynamic.h5')\n",
    "model_static = load_model('final_model_static.h5')\n",
    "scale_2class = pickle.load(open('Scale_2class.p','rb'))\n",
    "scale_static = pickle.load(open('Scale_static.p','rb'))\n",
    "scale_dynamic = pickle.load(open('Scale_dynamic.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "##scaling the data\n",
    "def transform_data(X,scale):\n",
    "    X_temp = X.reshape((X.shape[0] * X.shape[1], X.shape[2]))\n",
    "    X_temp = scale.transform(X_temp)\n",
    "    return X_temp.reshape(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "maXSpJMc4Ln5"
   },
   "source": [
    "## Evaluate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting output activity\n",
    "def predict_activity(X):\n",
    "    ##predicting whether dynamic or static\n",
    "    predict_2class = model_2class.predict(transform_data(X,scale_2class))\n",
    "    Y_pred_2class =  np.argmax(predict_2class, axis=1)\n",
    "    #static data filter\n",
    "    X_static = X[Y_pred_2class==1]\n",
    "    #dynamic data filter\n",
    "    X_dynamic = X[Y_pred_2class==0]\n",
    "    #predicting static activities\n",
    "    predict_static = model_static.predict(transform_data(X_static,scale_static))\n",
    "    predict_static = np.argmax(predict_static,axis=1)\n",
    "    #adding 4 because need to get final prediction lable as output\n",
    "    predict_static = predict_static + 4\n",
    "    #predicting dynamic activites\n",
    "    predict_dynamic = model_dynamic.predict(transform_data(X_dynamic,scale_dynamic))\n",
    "    predict_dynamic = np.argmax(predict_dynamic,axis=1)\n",
    "    #adding 1 because need to get final prediction lable as output\n",
    "    predict_dynamic = predict_dynamic + 1\n",
    "    ##appending final output to one list in the same sequence of input data\n",
    "    i,j = 0,0 \n",
    "    final_pred = []\n",
    "    for mask in Y_pred_2class:\n",
    "        if mask == 1:\n",
    "            final_pred.append(predict_static[i])\n",
    "            i = i + 1\n",
    "        else:\n",
    "            final_pred.append(predict_dynamic[j])\n",
    "            j = j + 1 \n",
    "    return final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "##predicting \n",
    "final_pred_val = predict_activity(X_val)\n",
    "final_pred_train = predict_activity(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of train data 0.9921109902067464\n",
      "Accuracy of validation data 0.9487614523243977\n"
     ]
    }
   ],
   "source": [
    "##accuracy of train and test\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy of train data',accuracy_score(Y_train,final_pred_train))\n",
    "print('Accuracy of validation data',accuracy_score(Y_val,final_pred_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best test accuracy obtained is 94.87% using divide and conquer."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ConquerDivide_HAR.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
